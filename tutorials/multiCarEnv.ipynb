{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4045b298",
   "metadata": {},
   "source": [
    "The following script is the environment created to simulate CACC behaviour, where there are $n$ number of follower cars in a line behind a leader car, and the goal is to train each car to follow a specific time headway between each other, avoiding collision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b64e9",
   "metadata": {},
   "source": [
    "To install the packages needed for this script, please refer to the README.md in the directory to instructions to create your virtual environment with *conda*. As well, to run this script itself, please select your virtual environment as the kernal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a477f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import pygame\n",
    "import functools\n",
    "\n",
    "from pygame import gfxdraw\n",
    "from gymnasium import spaces\n",
    "from pettingzoo.utils.env import ParallelEnv\n",
    "from pettingzoo.utils.conversions import parallel_wrapper_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198aa36e",
   "metadata": {},
   "source": [
    "Each of the *follower* cars are defined by an object of class *Car*. The *Car* object has the following properties: \n",
    "- Initial Position\n",
    "- Initial Velocity\n",
    "- Initial Acceleration \n",
    "- Position \n",
    "- Velocity \n",
    "- Acceleration \n",
    "\n",
    "Each of these properties help define the dynamics of the car, and are the same to the leader car dynamics. The leader car dynamics are defined in the *setup()* and *step()* function of the main environment class. \n",
    "\n",
    "Functions *__init__()* and *reset()* are used to create the object, define initial conditions, and to change the parameters back to the initial conditions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Car:\n",
    "    \n",
    "    # Initializes position and velocity of car \n",
    "    def __init__(self, initial_position, initial_velocity): \n",
    "        self.initial_position = initial_position\n",
    "        self.initial_velocity = initial_velocity\n",
    "        self.previous_acceleration = 0\n",
    "        self.reset()\n",
    "    \n",
    "    # Reset car state \n",
    "    def reset(self):\n",
    "        self.position = self.initial_position\n",
    "        self.velocity = self.initial_velocity\n",
    "        self.acceleration = 0.0\n",
    "        self.previous_acceleration = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a73675",
   "metadata": {},
   "source": [
    "Function *step()* applies the car dynamics per time step. Based on the environment time step *tau*, the car moves forward based on basic kinematics equations. The acceleration of the vehicles are determined by the action of the RL algorithm. This acceleration value is what you are trying to \"train\". The velocity is clipped to check for the bounds, and normalized to eliminate backwards movement.\n",
    "\n",
    "\n",
    "$$ v_1 = v_0 + at $$  \n",
    "$$ d = vt + \\frac{1}{2}at^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768884ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step in environment \n",
    "    def step(self, action, tau):\n",
    "        action = np.array(action).flatten()\n",
    "        self.previous_acceleration = self.acceleration\n",
    "\n",
    "        self.velocity = np.clip(self.velocity,0,33)\n",
    "        self.acceleration = action[0] * 3\n",
    "        self.velocity += self.acceleration*tau\n",
    "        self.normalizedVelocity = max(0,self.velocity)\n",
    "        self.position += self.normalizedVelocity * tau + 0.5 * self.acceleration * tau ** 2\n",
    "    # Return state \n",
    "    def get_state(self):\n",
    "        return np.array([self.position, self.velocity], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fad46",
   "metadata": {},
   "source": [
    "Most multi-agent reinforcement learning problems use PettingZoo to establish their environments. PettingZoo is the interface that allows your RL agents to learn, improve, and eventually change based on your dynamics and rewards. The general structure of the environment follows the following tutorial: [Custom Environment Tutorial](https://pettingzoo.farama.org/tutorials/custom_environment/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22f718",
   "metadata": {},
   "source": [
    "As well, there are several different types of multi-agent problems, and can be crudely described as either cooperative or competitive. This determines whether or not your agents will work together, or fight each other to achieve the best score. Our environment is cooperative, and the [Multiwalker](https://pettingzoo.farama.org/environments/sisl/multiwalker/) example was also used to model our environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45b0d5",
   "metadata": {},
   "source": [
    "To start, all the required parameters need to be initialized in the environment. This includes the number of followers, the timestep *τ* and basic common real-life parameters such as the length of the car. The velocity profile of the leader car is based on a dataset from the U.S Department of Transportation, where the velocity of cars were tracked across the [I-80 highway](https://datahub.transportation.gov/stories/s/Next-Generation-Simulation-NGSIM-Open-Data/i5zb-xe34/) in California. Each iteration of the environment selects a car from this dataset, and the leader car takes on their velocity profile. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541d045",
   "metadata": {},
   "source": [
    "As well, here you will see are your important spaces being defined: observation, action and agents. Note the upper and lower bounds of the actions and observations: they are normalized, ranging from 0 - 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e111ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of environment \n",
    "class ParallelCarEnv(ParallelEnv):\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\", \"debug\"], \"render_fps\": 60}\n",
    "\n",
    "    # Initializes the environment \n",
    "    def __init__(self, n_followers=2, render_mode=None): \n",
    "        super().__init__()\n",
    "\n",
    "        # Initial parameters \n",
    "        self.n_followers = n_followers\n",
    "        self.tau = 0.1\n",
    "        self.position_threshold = 1000 # 1 km \n",
    "        self.car_length = 5 # Length of the car in meters\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        \n",
    "        # Load velocity profiles\n",
    "        import os\n",
    "        data_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"data\", \"velocityProfiles.json\")\n",
    "        with open(data_path, \"r\") as f:\n",
    "            self.velocity_profiles = json.load(f)\n",
    "        self.unique_vehicle_ids = list(self.velocity_profiles.keys())\n",
    "\n",
    "        # Observation and Action Spaces\n",
    "        obs_low = np.array([0,0,0,0,0], dtype=np.float32) # [follower velocity, leader velocity, distance headway]\n",
    "        obs_high = np.array([1,1,1,1,1], dtype=np.float32) # [follower velocity, leader velocity, distance headway]\n",
    "\n",
    "        self.observation_spaces = {\n",
    "            f\"follower_{i}\": spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
    "            for i in range(self.n_followers)\n",
    "        }\n",
    "        self.action_spaces = {\n",
    "            f\"follower_{i}\": spaces.Box(\n",
    "                low=np.array([-1.0], dtype=np.float32),\n",
    "                high=np.array([1.0], dtype=np.float32),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            for i in range(self.n_followers)\n",
    "        }\n",
    "\n",
    "        # Generate agents (follower cars) \n",
    "        self._agents = [f\"follower_{i}\" for i in range(self.n_followers)]\n",
    "        self.possible_agents = self._agents.copy()\n",
    "\n",
    "    @property\n",
    "    def agents(self):\n",
    "        \"\"\"Return the list of active agents.\"\"\"\n",
    "        return self._agents.copy()\n",
    "    \n",
    "    @agents.setter\n",
    "    def agents(self, value):\n",
    "        \"\"\"Set the list of active agents.\"\"\"\n",
    "        self._agents = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3bfe64",
   "metadata": {},
   "source": [
    "The *reset()* function serves to restart the environment after an episode is done. The conditions for an episode reset are in the *step()* function. All environment and leader car parameters are changed to their initial values, and a new velocity profile is chosen for the leader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb386f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize / Reset the environment\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "        # Reset agents list to all possible agents\n",
    "        self._agents = self.possible_agents.copy()\n",
    "        \n",
    "        # Initial parameters \n",
    "        self.time = 0\n",
    "        self.leader_velocity_counter = 0\n",
    "        self.vehicle_id = random.choice(self.unique_vehicle_ids)\n",
    "        self.leader_velocity = self.velocity_profiles[self.vehicle_id][\"velocity\"][0] * 0.3048\n",
    "        self.leader_position = random.uniform(250,300)\n",
    "\n",
    "        # Reset followers \n",
    "        if hasattr(self, \"followers\") and len(self.followers) == self.n_followers:\n",
    "            # If followers already exist, reset their positions and velocities\n",
    "            for i, follower in enumerate(self.followers):\n",
    "                follower.initial_position = self.leader_position - 50 * (i + 1) - random.uniform(-5, 5)\n",
    "                follower.initial_velocity = np.clip(self.leader_velocity + random.uniform(-3, 3), 0, 33)\n",
    "                follower.reset()\n",
    "        else:\n",
    "            # Create new followers if they don't exist \n",
    "            self.followers = [\n",
    "                Car(\n",
    "                    initial_position=self.leader_position - 50 * (i + 1) - random.uniform(-5, 5),\n",
    "                    initial_velocity=np.clip(self.leader_velocity + random.uniform(-3, 3), 0, 33)\n",
    "                ) for i in range(self.n_followers)\n",
    "            ]       \n",
    "\n",
    "        # Reset all reward parameters \n",
    "        self.rewards = {agent: 0.0 for agent in self.agents}\n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "\n",
    "        observations = self._get_observations()\n",
    "        return observations, self.infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d4ed3",
   "metadata": {},
   "source": [
    "The *step()* function is where most of the actions happen in the environment. This is the part of the environment that takes actions from the RL agent to progress in the environment. From basic RL concepts, we know that the agent applies an action to take an environment from its current state to a new updated state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed82e85",
   "metadata": {},
   "source": [
    "The environment is first moved forward by increasing environment time by the defined timestep. The first main action we take is updating the leader vehicle. This is done by selecting the next value in the velocity dataset. This works because the dataset also has a 10ms rate, the same value as our *τ* value. After the leader moves forward, the follower cars also move. These dynamics can be seen the the respective *step()* function in the *Car* class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f55bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, actions):\n",
    "\n",
    "        self.time += self.tau # Move environment forward\n",
    "\n",
    "        # Update leader\n",
    "        if self.leader_velocity_counter < len(self.velocity_profiles[self.vehicle_id][\"velocity\"]):\n",
    "            self.leader_velocity = self.velocity_profiles[self.vehicle_id][\"velocity\"][self.leader_velocity_counter] * 0.3048 \n",
    "            self.leader_velocity_counter += 1\n",
    "        # If we've reached the end, keep the last velocity\n",
    "        self.leader_position += self.leader_velocity * self.tau\n",
    "\n",
    "        # Update followers\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            self.followers[i].step(actions[agent], self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17495e6",
   "metadata": {},
   "source": [
    "Next, the rewards are defined in the environment. The rewards are a crucial component that provides the agent feedback as to its performance. You use negative reward to indicate bad behaviour, and the contrary for good behaviour. The main characteristics of the rewards lie in the distance headway and the time headway between each car. These properties help us determine if the car is at a safe distance away from the car, which will influence the reward. The distance headway is determined as the distance from the back of the leader car to the front of the follower car. The time headway is found by using this distance headway, and the velocity of the follower vehicle. It is important to note the structure of the *self.followers* array: the first element of this array is the follower car right behind the leader car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de14be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rewards, terminations, truncations\n",
    "        #[ follower[2] --> follower[1] --> follower[0] --> leader ]\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if i == 0: # if it is the follower car right behind the leader \n",
    "                front_position = self.leader_position\n",
    "                velocity = self.leader_velocity\n",
    "            else: # if it is a follower car right behind another follower car \n",
    "                front_position = self.followers[i-1].position\n",
    "                velocity = self.followers[i-1].velocity\n",
    "\n",
    "            distanceHeadway = front_position - self.followers[i].position - self.car_length\n",
    "            timeHeadway = distanceHeadway / (self.followers[i].velocity + 1e-6) \n",
    "            relativeVelocity = self.followers[i].velocity - velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012afde3",
   "metadata": {},
   "source": [
    "Another important characteristic is the time to collision (TTC) threshold between the follower car and the car in front of it. This determines the time needed for the cars to collide. Given that the follower car is faster than the car in front, the TTC can be determined as:  \n",
    "$$ t_c(t) = \\frac{distance headway}{\\Delta v} $$\n",
    "\n",
    "The specific value of the reward, based on the TTC, is determined by: \n",
    "\n",
    "$$ r_c(t) = log\\left(\\frac{TTC}{TTC Threshold}\\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dfa74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time To Collision (TTC) Penalty\n",
    "            ttc_threshold = 4  # seconds\n",
    "            if relativeVelocity > 0:  # If follower is faster than the car in front\n",
    "                timeToCollision = distanceHeadway / relativeVelocity\n",
    "                if 0 < timeToCollision <= ttc_threshold: \n",
    "                    ttcPenalty = np.log10(timeToCollision/ttc_threshold); \n",
    "                else: \n",
    "                    ttcPenalty = 0\n",
    "            else:  # If follower is slower than the car in front \n",
    "                ttcPenalty = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4772c",
   "metadata": {},
   "source": [
    "With the following characteristics, values can be assigned based on the values of these characteristics. The only positive reward is the *lognorm* reward, which rewards the follower cars that keep a certain time headway between itself and the vehicle in front of it. This uses a log-normal probability distributions, which helps train the agent for a specific time headway. The target time headway here is 1.5 seconds. The probability distribution and [plot](https://www.desmos.com/calculator/xnucpurjxa) are shown here: \n",
    "\n",
    "$$ \\frac{1}{x\\sigma \\sqrt{2\\pi}} exp\\left(-\\frac{(ln(x) - \\mu)^2}{2\\sigma^2}\\right) $$\n",
    "\n",
    "The rest of the components of the reward function are penalties, and weightings are assigned based on the severity of thse penalties: \n",
    "- **ttc**: time to collision\n",
    "- **collision**: if two cars collide, high penalty\n",
    "- **jerk**: if a car changes their acceleration by a large margin, apply a penalty. We want smooth movement\n",
    "- **reverse**: we do not want any car moving backwards on a highway\n",
    "- **gap**: encourage the follower car to not be too far away from the leader car by applying a penalty if they are too far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward based on log-normal distribution (encouraging time headway ~ 1.5s)\n",
    "            normalized_timeHeadway = min(abs(timeHeadway), 15) \n",
    "            mew = 0.4226\n",
    "            sigma = 0.4365\n",
    "            x = max(1e-6, abs(normalized_timeHeadway))\n",
    "\n",
    "            # More balanced reward structure\n",
    "            lognorm = (10) * (1/(x*sigma*np.sqrt(2*np.pi)))*np.exp(-((np.log(x)-mew)**2)/(2*(sigma**2)))\n",
    "            ttc = (10) * ttcPenalty\n",
    "            collision = (-100) * (distanceHeadway <= 0)  # Reduced from -100\n",
    "            jerk = (-2) * abs(self.followers[i].previous_acceleration-self.followers[i].acceleration) / 6 \n",
    "            reverse = (-100) * (self.followers[i].velocity < 0)  # Reduced from -100   \n",
    "            gap = (-5) * (abs(timeHeadway) > 15 and distanceHeadway>100)\n",
    "            \n",
    "\n",
    "            reward = lognorm + ttc + collision + jerk + reverse + gap\n",
    "\n",
    "            self.rewards[agent] = reward\n",
    "            # Store reward breakdown in infos\n",
    "            if self.render_mode == \"debug\":\n",
    "                self.infos[agent] = {\n",
    "                    \"lognorm\": lognorm,\n",
    "                    \"ttc\": ttc,\n",
    "                    \"collision\": collision,\n",
    "                    \"jerk\": jerk,\n",
    "                    \"reverse\": reverse,\n",
    "                    \"gap\": gap\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453e95e",
   "metadata": {},
   "source": [
    "Now the environment determines if the episode should end. There are two types of ending conditions: truncation and termination. Truncation applies to events that are outside of environment constraints, such as a leader finishing its velocity profile. A termination is caused by catastrophic behaviour, such as collisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.rewards[agent] = reward\n",
    "            # Store reward breakdown in infos\n",
    "            if self.render_mode == \"debug\":\n",
    "                self.infos[agent] = {\n",
    "                    \"lognorm\": lognorm,\n",
    "                    \"ttc\": ttc,\n",
    "                    \"collision\": collision,\n",
    "                    \"jerk\": jerk,\n",
    "                    \"reverse\": reverse,\n",
    "                    \"gap\": gap\n",
    "                }\n",
    "\n",
    "            # Termination \n",
    "            if distanceHeadway <= 0: \n",
    "                self.terminations[agent] = True\n",
    "            elif abs(timeHeadway) > 15 and distanceHeadway > 100: \n",
    "                self.terminations[agent] = True\n",
    "            else:   \n",
    "                self.terminations[agent] = False\n",
    "                \n",
    "        shared_reward = np.mean(list(self.rewards.values()))\n",
    "        for agent in self.rewards:\n",
    "           self.rewards[agent] = shared_reward\n",
    "\n",
    "        # Global Truncation\n",
    "        for agent in self.agents:\n",
    "            if self.leader_velocity_counter >= len(self.velocity_profiles[self.vehicle_id][\"velocity\"]) or self.leader_position >= self.position_threshold:\n",
    "                self.truncations[agent] = True\n",
    "            else: \n",
    "                self.truncations[agent] = False \n",
    "        \n",
    "        # Global Termination\n",
    "        if any(self.terminations.values()) or any(self.truncations.values()):\n",
    "            for agent in self.agents:\n",
    "                self.terminations[agent] = True\n",
    "                self.truncations[agent] = self.truncations.get(agent, False)\n",
    "            self.agents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45cce2",
   "metadata": {},
   "source": [
    "At the end of the *step()* function, you want to render the environment if needed, and return the proper data, based on the PettingZoo API. Note the observations that are returned: a normalization function is used for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render environment \n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        \n",
    "        return self._get_observations(), self.rewards, self.terminations, self.truncations, self.infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f546c",
   "metadata": {},
   "source": [
    "The *_get_observations()* function serves to normalize the observation values. As the observation values grow over time in the environment, it is good practice to keep them within a range for easier training. The values used to normalize the values are the max values for the distance and velocity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79266afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized observations for each agent\n",
    "    def _get_observations(self):\n",
    "        observations = {}\n",
    "        for i, agent in enumerate(self.possible_agents):\n",
    "            if agent in self.agents:\n",
    "                front_position = self.leader_position if i == 0 else self.followers[i - 1].position\n",
    "                front_velocity = self.leader_velocity if i == 0 else self.followers[i - 1].velocity\n",
    "                state = self.followers[i].get_state()\n",
    "                distanceHeadway = front_position - self.followers[i].position - self.car_length\n",
    "                observations[agent] = np.array([\n",
    "                    front_position/self.position_threshold, # leader position\n",
    "                    front_velocity/33, # Max velocity is 33 m/s\n",
    "                    state[0]/self.position_threshold, # follower position\n",
    "                    state[1]/33, # Max velocity is 33 m/s\n",
    "                    distanceHeadway/self.position_threshold\n",
    "                ], dtype=np.float32) # normalized state\n",
    "            else:\n",
    "                # Return zeros for terminated/truncated agents\n",
    "                observations[agent] = np.zeros(self.observation_spaces[agent].shape, dtype=np.float32)\n",
    "        return observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a8b3b",
   "metadata": {},
   "source": [
    "For better training, these functions allocate cache for observation and action spacing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662bff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "    \n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5eb9d",
   "metadata": {},
   "source": [
    "Lastly, the *render()* function is important for visualizing performance after training: during training, be sure to set the mode to None. After initializing the required *pygame* modules, the background and general bounds are defined. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(self):\n",
    "        if self.render_mode is None:\n",
    "            # Default to rgb_array if not specified\n",
    "            self.render_mode = \"rgb_array\"\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError as e:\n",
    "            raise ImportError('pygame is not installed, run `pip install pygame`') from e\n",
    "\n",
    "        # Initialize pygame\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode((800, 600))\n",
    "                pygame.display.set_caption(\"multiCarEnv\")\n",
    "            elif self.render_mode == \"rgb_array\" or self.render_mode == \"debug\":\n",
    "                self.screen = pygame.Surface((800, 600))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        font = pygame.font.Font(None, 24)\n",
    "        world_width = self.position_threshold + 50\n",
    "        car_height = 25\n",
    "        car_width = 50\n",
    "        road_y = 400 \n",
    "\n",
    "        # Draw background\n",
    "        surf = pygame.Surface((800, 600))\n",
    "        surf.fill((255, 255, 255))\n",
    "        pygame.draw.line(surf, (0, 0, 0), (0, road_y), (800, road_y), 2)\n",
    "\n",
    "        meters_to_pixels = 800 / self.position_threshold\n",
    "        car_width = int(self.car_length * meters_to_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614a88d",
   "metadata": {},
   "source": [
    "Now the cars are generated. They are simple rectangles with a length of 5 (like in real life), and the coordinates of each car are the four corners of your car, where the \"position\" of the car is the front of the car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw leader car (red)\n",
    "        leader_x = int(self.leader_position * meters_to_pixels)\n",
    "        leader_coords = [\n",
    "            (leader_x - car_width, road_y - car_height), (leader_x - car_width, road_y - 2 * car_height),\n",
    "            (leader_x, road_y - 2 * car_height), (leader_x, road_y - car_height)\n",
    "        ]\n",
    "        gfxdraw.aapolygon(surf, leader_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(surf, leader_coords, (255, 0, 0))\n",
    "\n",
    "        # Draw followers (green)\n",
    "        for i, follower in enumerate(self.followers):\n",
    "            follower_x = int(follower.position * meters_to_pixels)\n",
    "            follower_coords = [\n",
    "                (follower_x - car_width, road_y - car_height), (follower_x - car_width, road_y - 2 * car_height),\n",
    "                (follower_x, road_y - 2 * car_height), (follower_x, road_y - car_height)\n",
    "            ]\n",
    "            gfxdraw.aapolygon(surf, follower_coords, (0, 0, 0))\n",
    "            gfxdraw.filled_polygon(surf, follower_coords, (0, 255, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea30c17",
   "metadata": {},
   "source": [
    "Lastly, different stats are shown on the simulation screen for easier testing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a694d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Draw follower stats\n",
    "            display_follower_velocity = f\"Follower {i} Velocity: {int(follower.velocity)} m/s\"\n",
    "            text_follower_velocity = font.render(display_follower_velocity, True, (0, 0, 0))\n",
    "            surf.blit(text_follower_velocity, (500, 95 + 35 * i))\n",
    "\n",
    "        # Draw leader stats\n",
    "        display_leader_velocity = f\"Leader Velocity: {int(self.leader_velocity)} m/s\"\n",
    "        text_leader_velocity = font.render(display_leader_velocity, True, (0, 0, 0))\n",
    "        surf.blit(text_leader_velocity, (500, 25))\n",
    "\n",
    "        display_leader_position = f\"Leader Position: {int(self.leader_position)} m\"\n",
    "        text_leader_position = font.render(display_leader_position, True, (0, 0, 0))\n",
    "        surf.blit(text_leader_position, (500, 60))\n",
    "\n",
    "        # Flip and blit\n",
    "        self.screen.blit(surf, (0, 0))\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    raise KeyboardInterrupt  # This will stop training\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "            return None  # Explicitly return None for human mode\n",
    "        elif self.render_mode == \"rgb_array\" or self.render_mode == \"debug\":\n",
    "            arr = np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "            return arr.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21312673",
   "metadata": {},
   "source": [
    "You will notice one last function in this environment. Traditionally for cooperative multi-agent environments, the PettingZoo Parallel API is used. For some PettingZoo utilities, conversion to their AEC API is needed, and a wrapper to do just this is needed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92947a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_env(*args, **kwargs):\n",
    "    return ParallelCarEnv(*args, **kwargs)\n",
    "\n",
    "# Agent API wrapper for PettingZoo compatibility\n",
    "from pettingzoo.utils.conversions import parallel_to_aec\n",
    "env = parallel_to_aec(ParallelCarEnv())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
